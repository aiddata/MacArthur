print(xtable(varMatrix, caption=var_caption, label="Variable_Table"),
size="footnotesize",
include.rownames=FALSE,
include.colnames=TRUE,
capiton.placement="bottom"
)
library(xtable)
print(xtable(varMatrix, caption=var_caption, label="Variable_Table"),
size="footnotesize",
include.rownames=FALSE,
include.colnames=TRUE,
capiton.placement="bottom"
)
varMatrix <- matrix(data=NA, nrow=length(names(vars)), ncol=2)
for (i in 1:length(names(vars)))
{
cur_var <- names(vars)[i]
short_name <-eval(parse(text= paste("vars$",cur_var,"$data_mini", sep="")))
varMatrix[i,] <- c(short_name, cur_var)
}
var_caption <- paste0("Variables used in this analysis.")
colnames(varMatrix) <- c("Short Name", "Long Name")
vars <- fromJSON(txt=json)
library(jsonlite)
vars <- fromJSON(txt=json)
varMatrix <- matrix(data=NA, nrow=length(names(vars)), ncol=2)
for (i in 1:length(names(vars)))
{
cur_var <- names(vars)[i]
short_name <-eval(parse(text= paste("vars$",cur_var,"$data_mini", sep="")))
varMatrix[i,] <- c(short_name, cur_var)
}
var_caption <- paste0("Variables used in this analysis.")
colnames(varMatrix) <- c("Short Name", "Long Name")
print(xtable(varMatrix, caption=var_caption, label="Variable_Table"),
size="footnotesize",
include.rownames=FALSE,
include.colnames=TRUE,
capiton.placement="bottom"
)
install.packages("papeR")
latex.table.cont(dta)
library(papeR)
latex.table.cont(dta)
install.packages(stargazer)
install.packages("stargazer")
stargazer(dta)
library(stargazer)
stargazer(dta)
names(dta2)
names(dta)
names(dta[6])
test <- dta[c(6,23:297)]
names(test)
t2 <- aggregate(test, by="OBJECTID")
t2 <- aggregate(test, by="OBJECTID", FUN=mean)
t2 <- aggregate(test, by=list(OBJECTID"", FUN=mean)
t2 <- aggregate(test, by=list(OBJECTID), FUN=mean)
t2 <- aggregate(test, by=list("OBJECTID"), FUN=mean)
t2 <- aggregate(test, by=list("OBJECTID"), FUN=mean, na.rm=TRUE)
install.packages("data.table")
head(dta2)
head(dta)
names(dta)
summary(dta$tc00_e)
colnames(dta)
?hist
mDir= getwd()
mDir= getwd()
getwd()
%tools>global options>sweave>weave rnw files using: knitr
%sudo apt-get install texlive
%sudo apt-get install texlive-latex-extra
%sudo apt-get install texlive-bibtex-extra
\documentclass{article}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[backend=bibtex, style=nature, citestyle=authoryear]{biblatex}
\usepackage[table]{xcolor}
\bibliography{WBVFM_IntroPar}
\newenvironment{knitrout}{}{}  %just a dummy environment
\makeatletter
\newcommand\gobblepars{%
\@ifnextchar\par%
{\expandafter\gobblepars\@gobble}%
{}}
\makeatother
\title{Descriptives: SEA and MacArthur}
\begin{document}
%\SweaveOpts{concordance=TRUE}
\begin{knitrout}
\maketitle
\tableofcontents
getwd()
library(RCurl)
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data"
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
filenames
getURL("ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data", ftp.use.epsv = FALSE, dirlistonly = TRUE)
getURL("ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/", ftp.use.epsv = FALSE, dirlistonly = TRUE)
files <- strsplit(filenames, "\r*\n")[[1]]
files
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
files <- strsplit(filenames, "\r*\n")[[1]]
files
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
files <- strsplit(filenames, "\r*\n")[[1]]
files
files[0]
files[1]
files[2]
files <- lapply(as.numeric, strsplit(filenames, "\r*\n")[[1]])
file_times <- as.numeric(files)
file_times
order(file_times)
file_times[order(file_times)]
file_times[order(-file_times)]
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
filenames = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
files_char <- strsplit(filenames, "\r*\n")[[1]]
files_int <- as.numeric(files_char)
file_order <- as.character(files_int[order(-files_int)])
file_order
file_active = file_order[1]
file_active
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
dir_names = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
dirs_char <- strsplit(dir_names, "\r*\n")[[1]]
dirs_int <- as.numeric(dirs_char)
dirs_order <- as.character(dirs_int[order(-dirs_int)])
dir_active = dirs_order[1]
# make new dir
active_dir_path = paste(mDir, "/analysis_data/", dir_active, sep='')
dir.create(active_dir_path, showWarnings = FALSE)
# get files
active_url <- paste(ftp_url, dir_active, '/', sep='')
file_str = getURL(active_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
file_names <- strsplit(file_str, "\r*\n")[[1]]
mDir = getwd()
# find latest data
ftp_url <- "ftp://reu:reudata@ftp.aiddata.wm.edu/REU/MacArthur/analysis_data/"
dir_names = getURL(ftp_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
dirs_char <- strsplit(dir_names, "\r*\n")[[1]]
dirs_int <- as.numeric(dirs_char)
dirs_order <- as.character(dirs_int[order(-dirs_int)])
dir_active = dirs_order[1]
# make new dir
active_dir_path = paste(mDir, "/analysis_data/", dir_active, sep='')
dir.create(active_dir_path, showWarnings = FALSE)
# get files
active_url <- paste(ftp_url, dir_active, '/', sep='')
file_str = getURL(active_url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
file_names <- strsplit(file_str, "\r*\n")[[1]]
for (i in 1:length(file_names)) {
print(i)
dst_file <- paste(active_dir_path, '/', file_names[[i]], sep="")
src_file <- paste(ftp_url, "/", file_names[[i]], sep="")
print(src_file)
download.file(src_file, destfile=dst_file, method="libcurl")
#     #copy the file without the revision history tags for use in the script.
#     name <- paste(strsplit(files[i], "_")[[1]][1],".",strsplit(strsplit(files[i], "_")[[1]][2], "[.]")[[1]][2], sep="")
#     new_name <- paste(mDir, "/analysis_data/", name, sep="")
#     file.copy(cur_file, new_name, overwrite=TRUE)
}
src_file <- paste(active_url, "/", file_names[[i]], sep="")
print(src_file)
file.exists(active_dir_path)
%tools>global options>sweave>weave rnw files using: knitr
%sudo apt-get install texlive
%sudo apt-get install texlive-latex-extra
%sudo apt-get install texlive-bibtex-extra
\documentclass{article}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[backend=bibtex, style=nature, citestyle=authoryear]{biblatex}
\usepackage[table]{xcolor}
\bibliography{WBVFM_IntroPar}
\newenvironment{knitrout}{}{}  %just a dummy environment
\makeatletter
\newcommand\gobblepars{%
\@ifnextchar\par%
{\expandafter\gobblepars\@gobble}%
{}}
\makeatother
\title{Descriptives: SEA and MacArthur}
\begin{document}
%\SweaveOpts{concordance=TRUE}
\begin{knitrout}
\maketitle
\tableofcontents
<<setup, echo=FALSE>>=
Sys.setenv(TEXTINPUTS=getwd(),
BIBINPUTS=getwd(),
BSTINPUTS=getwd())
render_sweave()
forest_thresh = 10
@
<<Libs, include=FALSE, cache=FALSE>>=
source("RLibs.R")
@
<<DataDownLoad, include=FALSE, cache=FALSE>>=
source("RDownload.R")
mDir = getwd()
active_dir_path  <- downlad_data(mDir)
@
<<DataDataFramesLoad, include=FALSE, cache=TRUE>>=
csv <- paste(active_dir_path, "/extracts/sea.csv", sep="")
json <- paste(active_dir_path, "/extracts/sea.json", sep="")
dta <- read.csv(csv)
vars <- fromJSON(txt=json)
#Cells (Unit of Obs)
dta2 <- dta[dta$NAME_0 == "Cambodia",]
#Spatial Cell Data
spdf_cells <- paste(active_dir_path, "/grids/sea_grid.shp", sep="")
cells <- readShapePoly(spdf_cells)
#Keep only relevant cells
AOI_cells <- sp::merge(cells, dta2, by="ID", all.x=FALSE)
AOI_cells<- AOI_cells[1:3000,]
proj4string(AOI_cells) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#SpatialADM data
spdf_adm_path <- paste(active_dir_path, "/ADM2/GADM_MacEcohotspotSubset_ADM2.shp", sep="")
spdf_adm <- readShapePoly(spdf_adm_path)
#Keep only relevant ADM data
spdf_adm <- spdf_adm[spdf_adm@data$NAME_0 == "Cambodia",]
location_csv <- paste(active_dir_path, "/MacArthur_Geocoded_data/locations.csv", sep="")
locations <- read.csv(location_csv)
locations2 <- locations[grep("Cambodia", locations$gazetteer_adm_name),]
coords = cbind(locations2$longitude, locations2$latitude)
#Aid
Mac_spdf <- SpatialPointsDataFrame(coords, locations2)
proj4string(Mac_spdf) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
@
\newpage
\textbf{WARNING:} Currently, only 3000 cells (about one-half of all cells) are being used in this analysis, due to computational limitations.  Please ignore the analysis below this line, for now.
\section{Data}
<<Variable_table, results='asis', echo=FALSE, cache=FALSE>>=
varMatrix <- matrix(data=NA, nrow=length(names(vars)), ncol=2)
for (i in 1:length(names(vars)))
{
cur_var <- names(vars)[i]
short_name <-eval(parse(text= paste("vars$",cur_var,"$data_mini", sep="")))
varMatrix[i,] <- c(short_name, cur_var)
}
var_caption <- paste0("Variables used in this analysis.")
colnames(varMatrix) <- c("Short Name", "Long Name")
print(xtable(varMatrix, caption=var_caption, label="Variable_Table"),
size="footnotesize",
include.rownames=FALSE,
include.colnames=TRUE,
capiton.placement="bottom"
)
@
Variable names are defined in Table \ref{Variable_Table}.
\newpage
\section{Descriptives}
%Cell-level statistics
<<Cell_Stats, results='asis', echo=FALSE>>=
#selecting just variables of interest
dta3 <- dta2[23:297]
#Select only the mean temperature and precip, for now.
colnames(dta3)[2] <- "per_loss_e"
dta3 <- dta3[,grep('e$',names(dta3), invert=FALSE)]
#Process dataframe to summarize temporal variables into single summaries
collapse_temporal <- function(x, prefix)
{
eval(parse(text=paste("t_dta <- x[,grep('^",prefix,"',names(x), invert=TRUE)]",sep="")))
eval(parse(text=paste("stat_dta <- x[,grep('^",prefix,"',names(x), invert=FALSE)]",sep="")))
eval(parse(text=paste("t_dta$",prefix,"_mean <- rowMeans(stat_dta)", sep="")))
return(t_dta)
}
stargaze_dta <- collapse_temporal(dta3, "lnyx")
stargaze_dta <- collapse_temporal(stargaze_dta, "ncc4")
stargaze_dta <- collapse_temporal(stargaze_dta, "at41")
stargaze_dta <- collapse_temporal(stargaze_dta, "pc41")
ttl = paste("Cell level descriptive statistics (N=",length(dta2[[1]]),")",sep="")
stargazer(stargaze_dta, title=ttl, notes="Variables with 'mean' appendix are summaries of the yearly mean values.")
@
%ADM2-level statistics
<<ADM_Stats, results='asis', echo=FALSE>>=
#selecting just variables of interest
dta3 <- dta2[c(6,23:297)]
colnames(dta3)[1] <- "OBJECTIDe"
colnames(dta3)[3] <- "per_loss_e"
dta3 <- dta3[,grep('e$',names(dta3), invert=FALSE)]
dta3_adm <- aggregate(dta3, by=list(dta3$OBJECTID), FUN=mean, na.rm=TRUE)
#Process dataframe to summarize temporal variables into single summaries
collapse_temporal <- function(x, prefix)
{
eval(parse(text=paste("t_dta <- x[,grep('^",prefix,"',names(x), invert=TRUE)]",sep="")))
eval(parse(text=paste("stat_dta <- x[,grep('^",prefix,"',names(x), invert=FALSE)]",sep="")))
eval(parse(text=paste("t_dta$",prefix,"_mean <- rowMeans(stat_dta)", sep="")))
return(t_dta)
}
stargaze_dta <- collapse_temporal(dta3_adm, "lnyx")
stargaze_dta <- collapse_temporal(stargaze_dta, "ncc4")
stargaze_dta <- collapse_temporal(stargaze_dta, "at41")
stargaze_dta <- collapse_temporal(stargaze_dta, "pc41")
stargaze_dta2 <- stargaze_dta[-c(1,2)]
ttl = paste("ADM2 level descriptive statistics (N=",length(unique(dta2$OBJECTID)),")",sep="")
stargazer(stargaze_dta2, title=ttl, notes=c("Variables with 'mean' appendix are summaries of the yearly mean values.", "All variables represent the mean cell value within a ADM 2."))
@
\begin{figure}[H]
\caption{Study area (ADM2) overlayed with Chinese development finance project locations.}
\label{LTDR_Hansen}
\centering
<<StudyAreaMap, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
plot(spdf_adm)
points(Mac_spdf, col="red")
@
\end{figure}
\begin{figure}[H]
\caption{Relationship between LTDR 2000 NDVI and Hansen \% Forest Cover 2000, of LTDR cells with at least \Sexpr{forest_thresh}\% forest cover according to Hansen 2000.}
\label{LTDR_Hansen}
\centering
<<LTDR_NDVI, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
ndviDTA <- dta2[c("tc00_e", "lnyx_2000e")]
ndviDTA_for <- ndviDTA[ndviDTA$tc00_e >= forest_thresh,]
plot(ndviDTA_for)
@
\end{figure}
The distribution of NDVI values for the remaining LTDR cells can be seen in figure \ref{LTDR_Hansen}.
As expected, we see a strong correlation - areas with higher levels of NDVI also tend to have higher levels of forest cover as estimated by Hansen.
Because LTDR and Hansen are not independent products (Hansen leverages the same input as LTDR in the product production), this analysis does not suggest that one product can be used to independently verify the other.
Rather, here we illustrate that historic LTDR trends can be relevant for establishing baselines when Hansen outcome variables are considered.
\begin{figure}[H]
\caption{Histogram of LTDR NDVI Values in (a) locations with >\Sexpr{forest_thresh}\% forest cover in Hansen 2000, and (b) locations with <\Sexpr{forest_thresh}\% forest cover in Hansen 2000.}
\label{NDVI_Hansen_hist}
\centering
<<LTDR_NDVI_hist, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
par(mfrow=c(2,1))
ndviDTA_for <- ndviDTA[ndviDTA$tc00_e >= forest_thresh,]
ttl = paste("Histogram of NDVI values in LTDR 5km w/ >",forest_thresh,"% Forest Cover", sep="")
hist(ndviDTA_for$lnyx_2000e, main=ttl, col="green", xlim=c(0,10000))
ndviDTA_notfor <- ndviDTA[ndviDTA$tc00_e <= forest_thresh,]
ttl2 = paste("Histogram of NDVI values in LTDR 5km w/ <",forest_thresh,"% Forest Cover", sep="")
hist(ndviDTA_notfor$lnyx_2000e, main=ttl2, col="red", xlim=c(0,10000))
@
\end{figure}
\section{Methods}
From each cell defined as forest (cells with > \Sexpr{forest_thresh}\% forest cover according to Hansen 2000), the euclidean distance (Haversine) to each chinese investment site (N = \Sexpr{length(locations2[[1]])}) is calculated and recorded.
<<Distance_Calc, echo=FALSE, cache=FALSE>>=
source("RDist.R")
dMatrix <- RDist(AOI_cells, Mac_spdf)
#In dMatrix, every column is a cell (referenced in order to AOI_cells)
#Every row is a MacArthur project (referenced in order to Mac_spdf)
#Average distance in KM:
avgDistKm <- mean(dMatrix) / 1000
#Minimum
col_mins <- do.call(pmin, lapply(1:nrow(dMatrix), function(i)dMatrix[i,]))
minDistKm <- mean(col_mins) / 1000
@
On average, the minimum distance between a unit of observation (the cell) and each chinese investment site is \Sexpr{round(minDistKm,0)}km, with an average distance of \Sexpr{round(avgDistKm,0)}km.
\par
DRAFT
To establish the degree of impact Chinese investment had on each grid cell, a weighted distance-decay function is approximated.
The spatial autocorrelation of forest tree cover is approximated using the 1999 LTDR dataset to avoid any potential confounds with treatments that start circa 2000.
By examining the spatial autocorrelation of forest tree cover, we hope to establish the maximum distance at which spillover effects might feasibly be observed (as the spatial pattern of tree cover in 1999 is the produce of all preceeding impacts).
Spatial autocorrelation is examined over 50 kilometer steps - for example, all cells within 50km are contrasted to one another, and the correlation of forest cover is recorded.
This is then repeated in bands, i.e., 50-100km; 100-150km and so forth until the 1000km limit is reached.
At each distance band, a summary measure of spatial autocorrelation - Morans I - is calculated, following:
\begin{equation}
I_h = (\frac{N}{\sum_{i}^{N}\sum_{j}^{N}w_{ij}}) * ((\sum_{i}^{N}\sum_{j}^{N}w_{ij} * (X_{i}-\bar{x}) * \frac{X_{j} - \bar{x}}{\sum_{i}^{N}(X_{i}-\bar{x})})^{2})
\end{equation}
where \textit{h} represents each spatial bin, \textit{N} the number of spatial units, \textit{i} and \textit{j} are indexes for each unit, \textit{X} is the variable of itnerest, and \begin{math}W_{ij}\end{math} represents the weights matrix.
In this application, the weights matrix is specified according to the bin (\textit{h}) being analyzed.
For example, if a chinese project is 100 kilometers from a cell of interest, the relevant spatial correlation estimated in this function is used to weight that project.
\\
<<Calc_correl, echo=FALSE, include=FALSE, cache=TRUE>>=
correlogram_data <- correlog(x = coordinates(AOI_cells)[,1], y = coordinates(AOI_cells)[,2], z=AOI_cells$lnyx_1999e, increment=5, latlon=TRUE, na.rm=TRUE, resamp=10)
#save data into a function to calculate the distance-decay penalty later.
#Chinese projects are "weighted" according to their distance.
#The absolulute correlation for a given distance is used as a weight.
#Projects at distances with a higher positive or inverse correlation are given the highest
#weights.
dVals<<- abs(correlogram_data$mean.of.class)
cVals <<- abs(correlogram_data$correlation)
@
\begin{figure}[H]
\caption{Distance decay of spatial autocorrelation observed in 1999 Forest Cover, as measured using the LTDR dataset.}
\label{LTDR_1999_Decay}
\centering
<<LTDR_1999_Decay, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
plot(correlogram_data)
@
\end{figure}
As this correlogram illustrates, a lack of positive spatial autocorrelation among LTDR measurements in 1999 is first observed at approximately \Sexpr{round(as.numeric(correlogram_data$x.intercept))} km.
We use this distance as a threshold to screen nearby Chinese aid projects - i.e., the average and sum distance of all Chinese aid projects within \Sexpr{round(as.numeric(correlogram_data$x.intercept))} km is taken for each cell, and projects beyond that distance are not included.
These two values - the average and summed distance of Chinese aid projects - are used as our approximation of the strength of Chinese interventions within any given cell on the landscape.
\par
<<Distance_thresholding, echo=FALSE>>=
#Strict threshold data-
#no weights, all before the first time X = 0 are counted.
thresh_dMatrix <- dMatrix
thresh_dMatrix[thresh_dMatrix > (as.numeric(correlogram_data$x.intercept)*1000)] <- NA
total_distance_km <- colSums(thresh_dMatrix, na.rm=TRUE) / 1000
AOI_cells$thresh_tot_proj <- apply(thresh_dMatrix, 2, function(x) length(which(!is.na(x))))
AOI_cells$thresh_totDist <- total_distance_km
AOI_cells$thresh_avgDist <- AOI_cells$thresh_totDist / AOI_cells$thresh_tot_proj
#distance decay
decay_dMatrix <- dMatrix
decay_dMatrix_adj <- apply(decay_dMatrix, 1:2, function(x){(cVals[which.min(abs(dVals - x))] * x)[[1]]})
AOI_cells$thresh_weightedDist <- colSums(decay_dMatrix_adj) / 1000
@
\begin{figure}[H]
\caption{Average Distance, Total Distance, and Total Count of Projects within \Sexpr{round(as.numeric(correlogram_data$x.intercept))} of each cell.}
\label{Proj_Avg_Dist}
grid.arrange(plot(spdf_adm),plot(spdf_adm),plot(spdf_adm),plot(spdf_adm))
grid.arrange(plot(spdf_adm),plot(spdf_adm),plot(spdf_adm),plot(spdf_adm))
?grid.arrange
library(qplot)
qplot
library(ggplot2)
qplot
test_plot <- qplot(AOI_cells@data$XMAX, AOI_cells@data$YMAX)
test_plot
grid.arrange(test_plot,test_plot,test_plot,test_plot)
grid.arrange(test_plot,test_plot,test_plot,test_plot, height=unit(0.5, "npc"))
theme.novpadding <-
list(layout.heights =
list(top.padding = 0,
main.key.padding = 0,
key.axis.padding = 0,
axis.xlab.padding = 0,
xlab.key.padding = 0,
key.sub.padding = 0,
bottom.padding = 0),
layout.widths =
list(left.padding = 0,
key.ylab.padding = 0,
ylab.axis.padding = 0,
axis.key.padding = 0,
right.padding = 0))
mapAvg = spplot(AOI_cells, "thresh_avgDist", main=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0), par.settings=theme.novpadding)
mapCnt = spplot(AOI_cells, "thresh_tot_proj", main=paste("Total Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0), par.settings=theme.novpadding)
mapTotDist =  spplot(AOI_cells, "thresh_totDist", main=paste("Total Distance of Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0), par.settings=theme.novpadding)
mapWeightedDist =  spplot(AOI_cells, "thresh_weightedDist", main="Weighted Distance \n of \n Chinese projects", sub="Distance Decay Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0), par.settings=theme.novpadding)
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist)
grid.arrange(test_plot,test_plot,test_plot,test_plot, height=unit(0.5, "npc"))
test_plot <- qplot(AOI_cells@data$XMAX, AOI_cells@data$YMAX)
grid.arrange(test_plot,test_plot,test_plot,test_plot, height=unit(0.5, "npc"))
test_plot <- qplot(AOI_cells@data$XMAX, AOI_cells@data$YMAX)
grid.arrange(test_plot,test_plot,test_plot,test_plot)
dat <- data.frame(x=1:10, y=10:1)
q1 <- qplot(x, y, data=dat)
q2 <- qplot(y, x, data=dat)
q3 <- qplot(x, y, data=dat, geom='line')
q4 <- qplot(y, x, data=dat, geom='line')
grid.arrange(q1, q2, q3, q4, heights=1:2, widths=1:2)
?units
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(4.5, "npc"),widths=units(3.5, "npc"))
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(4.5, "inches"),widths=unit(3.5, "inches"))
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(4.5, "inches"),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(2.5, "inches"),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapCnt = spplot(AOI_cells, "thresh_tot_proj", col="transparent", main=paste("Total Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapTotDist =  spplot(AOI_cells, "thresh_totDist", col="transparent", main=paste("Total Distance of Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapWeightedDist =  spplot(AOI_cells, "thresh_weightedDist", col="transparent", main="Weighted Distance \n of \n Chinese projects", sub="Distance Decay Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(2.5, "inches"),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
png(filename="mapAvgCnt.png")
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2, heights=unit(2.5, "inches"),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
?png
png(filename="mapAvgCnt.png", width=480, height=240)
grid.arrange(mapAvg, mapCnt, nrow=1, ncol=2)
dev.off()
png(filename="mapAvgCnt.png", width=600, height=240)
grid.arrange(mapAvg, mapCnt,ncol=2, nrow=1, heights=unit(2.5, "inches"), widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
png(filename="mapTotWeight.png")
grid.arrange(mapTotDist,mapWeightedDist, ncol=2, nrow=1, heights=unit(2.5, "inches"), widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
png(filename="mapFigs.png")
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
png(filename="mapFigs.png", width=7, height=5, units="inches")
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(units(2.5, "inches"),units(2.5, "inches")),widths=c(units(3.5, "inches"),units(3.5, "inches")))
dev.off()
png(filename="mapFigs.png", width=7, height=5, units="inches")
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
png(filename="mapFigs.png", width=7, height=5, units="in")
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
png(filename="mapFigs.png", width=7, height=5, units="in", res=300)
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=list(label=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""),cex=3), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapAvg
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=list(label=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""),cex=0.25), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapAvg
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=list(label=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""),cex=.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapAvg
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=list(label=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapCnt = spplot(AOI_cells, "thresh_tot_proj", col="transparent", main=list(label=paste("Total Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapTotDist =  spplot(AOI_cells, "thresh_totDist", col="transparent", main=list(label=paste("Total Distance of Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
mapWeightedDist =  spplot(AOI_cells, "thresh_weightedDist", col="transparent", main=list(label="Weighted Distance \n of \n Chinese projects",cex=0.75), sub="Distance Decay Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=1.0))
png(filename="mapFigs.png", width=7, height=5, units="in", res=300)
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
source("RLibs.R")
source("RLibs.R")
library(ncf)

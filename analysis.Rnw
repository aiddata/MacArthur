
%tools>global options>sweave>weave rnw files using: knitr

%sudo apt-get install texlive
%sudo apt-get install texlive-latex-extra 
%sudo apt-get install texlive-bibtex-extra
\documentclass{article}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[backend=bibtex, style=nature, citestyle=authoryear]{biblatex}
\usepackage[table]{xcolor}
\bibliography{WBVFM_IntroPar}
\newenvironment{knitrout}{}{}  %just a dummy environment
\makeatletter
\newcommand\gobblepars{%
    \@ifnextchar\par%
        {\expandafter\gobblepars\@gobble}%
        {}}
\makeatother
\title{Descriptives: SEA and MacArthur}
\begin{document}
%\SweaveOpts{concordance=TRUE}
\begin{knitrout}
<<setup, echo=FALSE>>=
Sys.setenv(TEXTINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
render_sweave()

forest_thresh = 10
rebuild_images = TRUE
restrict_analysis = FALSE
@

\maketitle
<<Pre_warning, echo=FALSE>>=
if(restrict_analysis != FALSE)
  {
  wartxt = paste("Currently, only ",restrict_analysis," cells are being used in this analysis, due to computational limitations.", sep="")
  }else
  {
  wartxt = "Draft Text."
  }
@
\textbf{WARNING:} \Sexpr{wartxt}
\tableofcontents



<<Libs, include=FALSE, cache=FALSE>>=
source("RLibs.R")
@

<<DataDownLoad, include=FALSE, cache=FALSE>>=
source("RDownload.R")
mDir = getwd()
active_dir_path  <- downlad_data(mDir)
@

<<DataDataFramesLoad, include=FALSE, cache=FALSE>>=
csv <- paste(active_dir_path, "/extracts/sea.csv", sep="")
json <- paste(active_dir_path, "/extracts/sea.json", sep="")
dta <- read.csv(csv)
vars <- fromJSON(txt=json)

#Cells (Unit of Obs)
dta2 <- dta[dta$NAME_0 == "Cambodia",]

#Spatial Cell Data
spdf_cells <- paste(active_dir_path, "/grids/sea_grid.shp", sep="")
cells <- readShapePoly(spdf_cells)
#Keep only relevant cells
AOI_cells <- sp::merge(cells, dta2, by="ID", all.x=FALSE)
if(restrict_analysis != FALSE)
{
AOI_cells<- AOI_cells[1:restrict_analysis,]
}
proj4string(AOI_cells) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")

#SpatialADM data
spdf_adm_path <- paste(active_dir_path, "/ADM2/GADM_MacEcohotspotSubset_ADM2.shp", sep="")
spdf_adm <- readShapePoly(spdf_adm_path)
#Keep only relevant ADM data
spdf_adm <- spdf_adm[spdf_adm@data$NAME_0 == "Cambodia",]

location_csv <- paste(active_dir_path, "/MacArthur_Geocoded_data/locations.csv", sep="")
locations <- read.csv(location_csv)
locations2 <- locations[grep("Cambodia", locations$gazetteer_adm_name),]
coords = cbind(locations2$longitude, locations2$latitude)

#Aid
Mac_spdf <- SpatialPointsDataFrame(coords, locations2)
proj4string(Mac_spdf) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#Join in the year of aid allocation
Mac_years <- paste(active_dir_path, "/MacArthur_Geocoded_data/projects.csv", sep="")
Mac_yr_dta <- read.csv(Mac_years)
Mac_spdf <- merge(Mac_spdf, Mac_yr_dta, by="project_id")
@

\newpage

\section{Data}
<<Variable_table, results='asis', echo=FALSE, cache=FALSE>>=
varMatrix <- matrix(data=NA, nrow=length(names(vars)), ncol=2)
for (i in 1:length(names(vars)))
{
  cur_var <- names(vars)[i]
  
  short_name <-eval(parse(text= paste("vars$",cur_var,"$data_mini", sep="")))
  varMatrix[i,] <- c(short_name, cur_var)
}

var_caption <- paste0("Variables used in this analysis.")

colnames(varMatrix) <- c("Short Name", "Long Name")

print(xtable(varMatrix, caption=var_caption, label="Variable_Table"), 
      size="footnotesize",
      include.rownames=FALSE,
      include.colnames=TRUE,
      capiton.placement="bottom"
      )
@
Variable names are defined in Table \ref{Variable_Table}.  

\newpage

\section{Descriptives}

%Cell-level statistics
<<Cell_Stats, results='asis', echo=FALSE>>=
#selecting just variables of interest
dta3 <- dta2[23:297]
#Select only the mean temperature and precip, for now.
colnames(dta3)[2] <- "per_loss_e"
dta3 <- dta3[,grep('e$',names(dta3), invert=FALSE)]

#Process dataframe to summarize temporal variables into single summaries
collapse_temporal <- function(x, prefix)
{
  eval(parse(text=paste("t_dta <- x[,grep('^",prefix,"',names(x), invert=TRUE)]",sep="")))
  eval(parse(text=paste("stat_dta <- x[,grep('^",prefix,"',names(x), invert=FALSE)]",sep="")))
  eval(parse(text=paste("t_dta$",prefix,"_mean <- rowMeans(stat_dta)", sep="")))
  return(t_dta)
}
stargaze_dta <- collapse_temporal(dta3, "lnyx")
stargaze_dta <- collapse_temporal(stargaze_dta, "ncc4")
stargaze_dta <- collapse_temporal(stargaze_dta, "at41")
stargaze_dta <- collapse_temporal(stargaze_dta, "pc41")
ttl = paste("Cell level descriptive statistics (N=",length(dta2[[1]]),")",sep="")
stargazer(stargaze_dta, title=ttl, notes="Variables with 'mean' appendix are summaries of the yearly mean values.")
@

%ADM2-level statistics
<<ADM_Stats, results='asis', echo=FALSE>>=
#selecting just variables of interest
dta3 <- dta2[c(6,23:297)]
colnames(dta3)[1] <- "OBJECTIDe"
colnames(dta3)[3] <- "per_loss_e"
dta3 <- dta3[,grep('e$',names(dta3), invert=FALSE)]

dta3_adm <- aggregate(dta3, by=list(dta3$OBJECTID), FUN=mean, na.rm=TRUE)

#Process dataframe to summarize temporal variables into single summaries
collapse_temporal <- function(x, prefix)
{
  eval(parse(text=paste("t_dta <- x[,grep('^",prefix,"',names(x), invert=TRUE)]",sep="")))
  eval(parse(text=paste("stat_dta <- x[,grep('^",prefix,"',names(x), invert=FALSE)]",sep="")))
  eval(parse(text=paste("t_dta$",prefix,"_mean <- rowMeans(stat_dta)", sep="")))
  return(t_dta)
}
stargaze_dta <- collapse_temporal(dta3_adm, "lnyx")
stargaze_dta <- collapse_temporal(stargaze_dta, "ncc4")
stargaze_dta <- collapse_temporal(stargaze_dta, "at41")
stargaze_dta <- collapse_temporal(stargaze_dta, "pc41")
stargaze_dta2 <- stargaze_dta[-c(1,2)]
ttl = paste("ADM2 level descriptive statistics (N=",length(unique(dta2$OBJECTID)),")",sep="")
stargazer(stargaze_dta2, title=ttl, notes=c("Variables with 'mean' appendix are summaries of the yearly mean values.", "All variables represent the mean cell value within a ADM 2."))
@
\begin{figure}[H]
\caption{Study area (ADM2) overlayed with Chinese development finance project locations.}
\label{LTDR_Hansen}
\centering
<<StudyAreaMap, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
plot(spdf_adm)
points(Mac_spdf, col="red")
@
\end{figure}

\begin{figure}[H]
\caption{Relationship between LTDR 2000 NDVI and Hansen \% Forest Cover 2000, of LTDR cells with at least \Sexpr{forest_thresh}\% forest cover according to Hansen 2000.}
\label{LTDR_Hansen}
\centering
<<LTDR_NDVI, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
ndviDTA <- dta2[c("tc00_e", "lnyx_2000e")]
ndviDTA_for <- ndviDTA[ndviDTA$tc00_e >= forest_thresh,]
plot(ndviDTA_for)
@
\end{figure}
The distribution of NDVI values for the remaining LTDR cells can be seen in figure \ref{LTDR_Hansen}.  
As expected, we see a strong correlation - areas with higher levels of NDVI also tend to have higher levels of forest cover as estimated by Hansen.  
Because LTDR and Hansen are not independent products (Hansen leverages the same input as LTDR in the product production), this analysis does not suggest that one product can be used to independently verify the other.  
Rather, here we illustrate that historic LTDR trends can be relevant for establishing baselines when Hansen outcome variables are considered.
\begin{figure}[H]
\caption{Histogram of LTDR NDVI Values in (a) locations with >\Sexpr{forest_thresh}\% forest cover in Hansen 2000, and (b) locations with <\Sexpr{forest_thresh}\% forest cover in Hansen 2000.}
\label{NDVI_Hansen_hist}
\centering
<<LTDR_NDVI_hist, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
par(mfrow=c(2,1))
ndviDTA_for <- ndviDTA[ndviDTA$tc00_e >= forest_thresh,]
ttl = paste("Histogram of NDVI values in LTDR 5km w/ >",forest_thresh,"% Forest Cover", sep="")
hist(ndviDTA_for$lnyx_2000e, main=ttl, col="green", xlim=c(0,10000))
ndviDTA_notfor <- ndviDTA[ndviDTA$tc00_e <= forest_thresh,]
ttl2 = paste("Histogram of NDVI values in LTDR 5km w/ <",forest_thresh,"% Forest Cover", sep="")
hist(ndviDTA_notfor$lnyx_2000e, main=ttl2, col="red", xlim=c(0,10000))
@
\end{figure}
\section{Methods}

From each cell defined as forest (cells with > \Sexpr{forest_thresh}\% forest cover according to Hansen 2000), the euclidean distance (Haversine) to each chinese investment site (N = \Sexpr{length(locations2[[1]])}) is calculated and recorded.  
<<Distance_Calc, echo=FALSE, cache=FALSE>>=
source("RDist.R")
dMatrix <- RDist(AOI_cells, Mac_spdf)
#In dMatrix, every column is a cell (referenced in order to AOI_cells)
#Every row is a MacArthur project (referenced in order to Mac_spdf)

#Average distance in KM:
avgDistKm <- mean(dMatrix) / 1000

#Minimum
col_mins <- do.call(pmin, lapply(1:nrow(dMatrix), function(i)dMatrix[i,]))
minDistKm <- mean(col_mins) / 1000

@
On average, the minimum distance between a unit of observation (the cell) and each chinese investment site is \Sexpr{round(minDistKm,0)}km, with an average distance of \Sexpr{round(avgDistKm,0)}km.
\par
To establish the degree of impact Chinese investment had on each grid cell, a weighted distance-decay function is approximated.  
The spatial autocorrelation of forest tree cover is approximated using the 1999 LTDR dataset to avoid any potential confounds with treatments that start circa 2000.
By examining the spatial autocorrelation of forest tree cover, we hope to establish the maximum distance at which spillover effects might feasibly be observed (as the spatial pattern of tree cover in 1999 is the produce of all preceeding impacts).
Spatial autocorrelation is examined over 50 kilometer steps - for example, all cells within 50km are contrasted to one another, and the correlation of forest cover is recorded.  
This is then repeated in bands, i.e., 50-100km; 100-150km and so forth until the 1000km limit is reached. 
At each distance band, a summary measure of spatial autocorrelation - Morans I - is calculated, following:
\begin{equation}
I_h = (\frac{N}{\sum_{i}^{N}\sum_{j}^{N}w_{ij}}) * ((\sum_{i}^{N}\sum_{j}^{N}w_{ij} * (X_{i}-\bar{x}) * \frac{X_{j} - \bar{x}}{\sum_{i}^{N}(X_{i}-\bar{x})})^{2})
\end{equation}
where \textit{h} represents each spatial bin, \textit{N} the number of spatial units, \textit{i} and \textit{j} are indexes for each unit, \textit{X} is the variable of itnerest, and \begin{math}W_{ij}\end{math} represents the weights matrix.  
In this application, the weights matrix is specified according to the bin (\textit{h}) being analyzed.  
For example, if a chinese project is 100 kilometers from a cell of interest, the relevant spatial correlation estimated in this function is used to weight that project.
\\

<<Calc_correl, echo=FALSE, include=FALSE, cache=FALSE>>=
correlogram_data <- correlog(x = coordinates(AOI_cells)[,1], y = coordinates(AOI_cells)[,2], z=AOI_cells$lnyx_1999e, increment=5, latlon=TRUE, na.rm=TRUE, resamp=50)

#save data into a function to calculate the distance-decay penalty later.
#Chinese projects are "weighted" according to their distance.
#The absolulute correlation for a given distance is used as a weight.
#Projects at distances with a higher positive or inverse correlation are given the highest
#weights.

dVals <<- abs(correlogram_data$mean.of.class)
cVals <<- abs(correlogram_data$correlation)

@
  \begin{figure}[H]
  \caption{Distance decay of spatial autocorrelation observed in 1999 Forest Cover, as measured using the LTDR dataset.}
\label{LTDR_1999_Decay}
\centering
<<LTDR_1999_Decay, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
plot(correlogram_data)
@
\end{figure}

As this correlogram illustrates, a lack of positive spatial autocorrelation among LTDR measurements in 1999 is first observed at approximately \Sexpr{round(as.numeric(correlogram_data$x.intercept))} km.  
We use this distance as a threshold to screen nearby Chinese aid projects - i.e., the average and sum distance of all Chinese aid projects within \Sexpr{round(as.numeric(correlogram_data$x.intercept))} km is taken for each cell, and projects beyond that distance are not included.  
These two values - the average and summed distance of Chinese aid projects - are used as our approximation of the strength of Chinese interventions within any given cell on the landscape.
\par

<<Distance_thresholding, echo=FALSE>>=
#Strict threshold data-
#no weights, all before the first time X = 0 are counted.
thresh_dMatrix <- dMatrix
thresh_dMatrix[thresh_dMatrix > (as.numeric(correlogram_data$x.intercept)*1000)] <- NA
total_distance_km <- colSums(thresh_dMatrix, na.rm=TRUE) / 1000

AOI_cells$thresh_tot_proj <- apply(thresh_dMatrix, 2, function(x) length(which(!is.na(x))))
AOI_cells$thresh_totDist <- total_distance_km
AOI_cells$thresh_avgDist <- AOI_cells$thresh_totDist / AOI_cells$thresh_tot_proj 

#distance decay
decay_dMatrix <- dMatrix
decay_dMatrix_adj <- apply(decay_dMatrix, 1:2, function(x){(cVals[which.min(abs(dVals - x))] * x)[[1]]})
AOI_cells$thresh_weightedDist <- colSums(decay_dMatrix_adj) / 1000

@


  \begin{figure}[H]
  \caption{Potential measurements of treatment indicator.}
\label{Proj_Avg_Dist}
\centering
<<Treatment_Map, echo=FALSE, cache=FALSE>>=
mapAvg = spplot(AOI_cells, "thresh_avgDist", col="transparent", main=list(label=paste("Average Distance \n (within ",round(correlogram_data$x.intercept)," km) \n to Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=0.5))
mapCnt = spplot(AOI_cells, "thresh_tot_proj", col="transparent", main=list(label=paste("Total Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=0.5))
mapTotDist =  spplot(AOI_cells, "thresh_totDist", col="transparent", main=list(label=paste("Total Distance of Nearby \n (within ",round(correlogram_data$x.intercept)," km) \n Chinese projects", sep=""),cex=0.75), sub="Threshold Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=0.5))
mapWeightedDist =  spplot(AOI_cells, "thresh_weightedDist", col="transparent", main=list(label="Weighted Distance \n of \n Chinese projects",cex=0.75), sub="Distance Decay Approach", sp.layout=list(Mac_spdf, pch=16, col="red", cex=0.5))

if(rebuild_images == TRUE)
{
png(filename="mapFigs.png", width=7, height=5, units="in", res=300)
grid.arrange(mapAvg, mapCnt, mapTotDist,mapWeightedDist, nrow=2, ncol=2, heights=c(unit(2.5, "inches"),unit(2.5, "inches")),widths=c(unit(3.5, "inches"),unit(3.5, "inches")))
dev.off()
}
@
\includegraphics{mapFigs}
\end{figure}



<<Treatment_Over_Time, echo=FALSE, cache=FALSE>>=
all_years <- unique(Mac_spdf$transactions_start_year)
all_years <- all_years[!is.na(all_years)]

dYears <- list()
#Drop all MacArthur projects that have no start or end date.
Mac_spdf <- Mac_spdf[!is.na(Mac_spdf$transactions_start_year),]

for(years in 1:length(all_years))
{
  year <- all_years[years]
  ThisYearMac <- Mac_spdf[Mac_spdf@data$transactions_start_year == year,]
  dYears[[years]] <- RDist(AOI_cells, ThisYearMac)
}

#In dMatrix, every column is a cell (referenced in order to AOI_cells)
#Every row is a MacArthur project for that year (referenced in order to Mac_spdf)

AvgYears <- vector()
for(years in 1:length(all_years))
{
AvgYears[[years]] <- mean(dYears[[years]]) / 1000
}

Avg_MinYears <- vector()
for(years in 1:length(all_years))
{
col_mins_year <- do.call(pmin, lapply(1:nrow(dYears[[years]]), function(i)dYears[[years]][i,]))
nameRef <- paste("MinYr_",all_years[years], sep="")
AOI_cells@data[nameRef] <- col_mins_year / 1000
Avg_MinYears[[years]] <- mean(col_mins_year) / 1000
}

Dist_Decay_Yrs <- vector()
for(years in 1:length(all_years))
{
t_dyears <- dYears[[years]] / 1000
decay_dMatrix_adj <- apply(t_dyears, 1:2, function(x){(cVals[which.min(abs(dVals - x))] * x)[[1]]})
nameRef <- paste("DecayYr_",all_years[years], sep="")
AOI_cells@data[nameRef] <- colMeans(decay_dMatrix_adj) / 1000
Dist_Decay_Yrs[[years]] <- mean(colMeans(decay_dMatrix_adj) / 1000)
}

CountProj_Years <- vector()
for(years in 1:length(all_years))
{
CountProj_Years[[years]] <- nrow(dYears[[years]])
}



#Build a quick temporal dataframe for plotting and ordering
TempDF <- cbind.data.frame(all_years, AvgYears, Avg_MinYears, CountProj_Years, Dist_Decay_Yrs)
TempDF <- TempDF[with(TempDF, order(TempDF[,1])),]
@

  \begin{figure}[H]
  \caption{Potential measurements of treatment indicator.}
\label{Temporal_Measurements}
\centering
<<OverTime_Treatment_Fig, echo=FALSE, fig="TRUE", out.width="0.8\\linewidth">>=
par(mfrow=c(2,2))
plot(TempDF$all_years, TempDF$Dist_Decay_Yrs, type="b", lty=2, xlab="Year", ylab="Avg. Dist. Weighted Est. Impact of Chinese Aid")
plot(TempDF$all_years, TempDF$AvgYears, type="b", lty=2, xlab="Year", ylab="Avg. Dist. to New Chinese Aid Proj.")
plot(TempDF$all_years, TempDF$CountProj_Years, type="b", lty=2, xlab="Year", ylab="Count of New Chinese Aid Proj.")
plot(TempDF$all_years, TempDF$Avg_MinYears, type="b", lty=2, xlab="Year", ylab="Avg. Dist. to Closest New Chinese Aid Proj.")

@
\end{figure}
Figure \ref{Proj_Avg_Dist} shows an example of the treatment variables examined in this analysis, averaged across the full time period, while \ref{Temporal_Measurements} shows the overall fluctuations of these measurements over time, across all grid cells being examined.

<<Analysis, echo=FALSE>>=

#Very prelimary analysis placeholder to show functional form.
#Drop the spatial DF
DFa <- AOI_cells@data
#Drop irrelevant variables:
dropvars <- c("XMIN","XMAX","YMIN","YMAX","OBJECTID","ID_0","ISO","NAME_0","HASC_2","ID_1","NAME_1","NAME_2","CCN_2","CCA_2","TYPE_2","ENGTYPE_2","NL_NAME_2","VARNAME_2","Shape_Leng","Shape_Area", "thresh_tot_proj","thresh_totDist","thresh_avgDist","thresh_weightedDist")

DFa <- DFa[,!(names(DFa) %in% dropvars)]

DFa <- DFa[, -grep("(19)", names(DFa))]

#Prep for wide to long translation
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

rename_header <- function(x,sub)
{
t <- paste(substr(x, 1, 0), sub, substr(x, 1, nchar(x)), sep = "")
substr(t, 1, nchar(t)-1)
}

names(DFa)[25:143]<- sapply(names(DFa)[25:143], function(x) {rename_header(x, substrRight(x,1))})

names(DFa)[6:19] <- sapply(names(DFa)[6:19], function(x){substr(x, 5, nchar(x))})

#drop data for the year 2000, 2013-2015 (NTL ends in 2012)
DFa <- DFa[, -grep("(2000)", names(DFa))]
DFa <- DFa[, -grep("(2015)", names(DFa))]
DFa <- DFa[, -grep("(2013)", names(DFa))]
DFa <- DFa[, -grep("(2014)", names(DFa))]

PCloss <- grep("^loss", names(DFa))
mean_ln <- grep("^elnyx", names(DFa))
NTL <- grep("^encc4", names(DFa))
minairTemp <- grep("^mat41", names(DFa))
maxairTemp <- grep("^xat41", names(DFa))
meanairTemp <- grep("^eat41", names(DFa))
minPre <- grep("^mpc41", names(DFa))
maxPre <- grep("^xpc41", names(DFa))
meanPre <- grep("^epc41", names(DFa))
MinDist <- grep("^MinYr", names(DFa))
DecayDist <- grep("^DecayYr", names(DFa))

all_reshape <- c(PCloss, mean_ln, NTL, minairTemp, maxairTemp, meanairTemp, minPre, maxPre, meanPre, MinDist, DecayDist)




DFa <- reshape(DFa, varying=all_reshape,direction="long", idvar="ID", sep="_", timevar="Year")

#Rename names to something interpretable...

names(DFa)[names(DFa) == "ID_2"] = "District"
names(DFa)[names(DFa) == "loss"] = "Forest_Loss"
names(DFa)[names(DFa) == "encc4"] = "NighttimeLights"
names(DFa)[names(DFa) == "mat41"] = "MinTemp"
names(DFa)[names(DFa) == "xat41"] = "MaxTemp"
names(DFa)[names(DFa) == "eat41"] = "MeanTemp"
names(DFa)[names(DFa) == "mpc41"] = "MinPrecip"
names(DFa)[names(DFa) == "xpc41"] = "MaxPrecip"
names(DFa)[names(DFa) == "epc41"] = "MeanPrecip"


initModel <- lm(Forest_Loss ~ DecayYr + NighttimeLights + MinTemp + MaxTemp + MeanTemp + MaxPrecip + MeanPrecip + MinPrecip + factor(District), data=DFa)
cluster <- cluster.vcov(initModel, cbind(DFa$Year, DFa$ID_2), force_posdef=TRUE)
CMREG <- coeftest(initModel, cluster)
@
Functional Form of Analysis:
\begin{equation}
Hansen_{it} = \alpha + \theta * WeightedDistanceActiveChineseProject_{it} + \sum(\beta_{j}*X_{j}) + D_{region} + D_{t} + D_{region} * t + \epsilon_{it}
%\frac{1}{(2\pi)^{1/2}*\sigma_{i}} * exp(-\frac{1}{2}*\frac{(x-\mu_{i})^{2}}{\sigma_{i}^{2}})
\end{equation}

<<Model_Results, results='asis', echo=FALSE>>=
stargazer(CMREG, title="Initial Results", keep=c("Forest_Loss", "NighttimeLights","MinTemp","MaxTemp","MeanTemp", "MinPrecip", "MaxPrecip", "MeanPrecip"))
@

\end{knitrout}
\end{document}
